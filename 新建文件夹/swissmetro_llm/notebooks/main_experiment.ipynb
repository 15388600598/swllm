{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Swissmetro LLM-based Synthetic Data Generation\n",
    "\n",
    "## 整体思路\n",
    "\n",
    "### 三条LLM生成路线\n",
    "\n",
    "| 路线 | 输出变量 | LLM输出内容 | 采样方法 | 关键参数 |\n",
    "|------|----------|-------------|----------|----------|\n",
    "| **A. Two-stage CHOICE** | `syn_final` | CHOICE (1/2/3) | 直接取值 + 修复 | low_prob_threshold=0.01 |\n",
    "| **B. Utility + Softmax** | `syn_u` | U_TRAIN/U_SM/U_CAR | softmax(U/tau)采样 | tau ∈ [0.2, 0.5, 0.8, 1.0, 1.2] |\n",
    "| **C. MNL + Residual** | `syn_r` | dU ∈ [-1,1] | logits + λ*dU → softmax | λ ∈ [0.1, 0.2, 0.3, 0.5] |\n",
    "\n",
    "### 基准MNL的作用\n",
    "- **是** Step2(main) MNL（含MALE/INCOME/AGE/PURPOSE交互项，42个参数）\n",
    "- 用于：P_CHOSEN计算、bad_idx筛查、evaluate_one指标、参数稳定性比较"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path if running from notebooks folder\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import swissmetro_llm modules\n",
    "from swissmetro_llm import config\n",
    "from swissmetro_llm.data import load_swissmetro, build_matrices, split_train_test_by_id\n",
    "from swissmetro_llm.data.preprocessing import compute_scales_from_train, apply_scales\n",
    "from swissmetro_llm.models import (\n",
    "    fit_mnl_step1, predict_step1, fit_mnl_step2, predict_step2,\n",
    "    build_X_ind, cluster_bootstrap_thetas, compute_bootstrap_se,\n",
    "    fit_step2_main, get_step2_main_param_names\n",
    ")\n",
    "from swissmetro_llm.models.utils import accuracy, neg_loglike_from_P, softmax_rows\n",
    "from swissmetro_llm.evaluation import (\n",
    "    evaluate_one, feasibility_min, diversity_report,\n",
    "    score_with_baseline_mnl, downstream_metrics\n",
    ")\n",
    "from swissmetro_llm.generation import (\n",
    "    create_templates, generate_from_utilities_batch,\n",
    "    generate_from_residual_batch, generate_two_stage\n",
    ")\n",
    "from swissmetro_llm.stability import (\n",
    "    make_augmented, stability_analysis, format_stability_for_report\n",
    ")\n",
    "\n",
    "print(\"Package loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data path here\n",
    "DATA_PATH = \"../swissmetro.dat\"  # UPDATE THIS PATH if needed\n",
    "\n",
    "# Load data\n",
    "df = load_swissmetro(DATA_PATH)\n",
    "print(f\"Loaded {len(df)} rows, {df['ID'].nunique()} respondents\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test by respondent ID\n",
    "df_train, df_test, train_ids, test_ids = split_train_test_by_id(df, test_size=0.2, seed=42)\n",
    "\n",
    "print(f\"Train: {len(df_train)} rows, {len(train_ids)} respondents\")\n",
    "print(f\"Test: {len(df_test)} rows, {len(test_ids)} respondents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Build Matrices and Compute Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build matrices for MNL estimation\n",
    "train_data = build_matrices(df_train)\n",
    "test_data = build_matrices(df_test)\n",
    "\n",
    "# Compute scales from training data only\n",
    "tt_scale, co_scale, he_scale = compute_scales_from_train(train_data)\n",
    "\n",
    "print(f\"TT scale: {tt_scale}\")\n",
    "print(f\"CO scale: {co_scale}\")\n",
    "print(f\"HE scale: {he_scale}\")\n",
    "\n",
    "# Apply scales\n",
    "train_data = apply_scales(train_data, tt_scale, co_scale, he_scale)\n",
    "test_data = apply_scales(test_data, tt_scale, co_scale, he_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. MNL Model Estimation (Step1 + Step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Basic MNL with level-of-service attributes (6 parameters)\n",
    "print(\"=\" * 50)\n",
    "print(\"Step 1: Basic MNL (6 parameters)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "res1 = fit_mnl_step1(train_data)\n",
    "\n",
    "print(f\"Converged: {res1.success}\")\n",
    "print(f\"Neg log-likelihood: {res1.fun:.2f}\")\n",
    "\n",
    "theta1 = res1.x\n",
    "param_names1 = [\"B_TT\", \"B_CO\", \"B_HE\", \"B_SEATS\", \"ASC_SM\", \"ASC_CAR\"]\n",
    "print(\"\\nParameters:\")\n",
    "for name, val in zip(param_names1, theta1):\n",
    "    print(f\"  {name:12s}: {val:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extended MNL with individual characteristics (42 parameters)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Step 2: Extended MNL with interactions (42 parameters)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X_train = build_X_ind(train_data)\n",
    "X_test = build_X_ind(test_data)\n",
    "\n",
    "res2 = fit_mnl_step2(train_data, X_train, theta1=theta1, maxiter=20000, maxfun=20000)\n",
    "\n",
    "print(f\"Converged: {res2.success}\")\n",
    "print(f\"Neg log-likelihood: {res2.fun:.2f}\")\n",
    "print(f\"Number of parameters: {len(res2.x)}\")\n",
    "\n",
    "theta2 = res2.x\n",
    "\n",
    "# Note: Step2 may not converge but parameters are still usable if accuracy is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on train and test\n",
    "P_train = predict_step2(theta2, train_data, X_train)\n",
    "P_test = predict_step2(theta2, test_data, X_test)\n",
    "\n",
    "print(f\"Train accuracy: {accuracy(P_train, train_data['y']):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy(P_test, test_data['y']):.3f}\")\n",
    "print(f\"Train LL: {-neg_loglike_from_P(P_train, train_data['y']):.2f}\")\n",
    "print(f\"Test LL: {-neg_loglike_from_P(P_test, test_data['y']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Bootstrap Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clustered bootstrap\n",
    "# Set B=30 for quick test, B=200 for publication\n",
    "B = 30\n",
    "\n",
    "print(f\"Running {B} bootstrap iterations...\")\n",
    "print(\"(This may take several minutes)\")\n",
    "\n",
    "thetas_boot = cluster_bootstrap_thetas(\n",
    "    df_train, \n",
    "    train_ids,\n",
    "    tt_scale, co_scale, he_scale,\n",
    "    theta_init=theta2,\n",
    "    B=B,\n",
    "    seed=2025,\n",
    "    maxiter=20000,\n",
    "    maxfun=20000,\n",
    "    verbose_every=5\n",
    ")\n",
    "\n",
    "print(f\"\\nBootstrap completed: {thetas_boot.shape[0]} / {B} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bootstrap standard errors and confidence intervals\n",
    "from swissmetro_llm.models.bootstrap import compute_bootstrap_ci\n",
    "\n",
    "se_boot = compute_bootstrap_se(thetas_boot)\n",
    "ci_lower, ci_upper = compute_bootstrap_ci(thetas_boot, alpha=0.05)\n",
    "\n",
    "print(\"First 6 parameters with bootstrap inference:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Param':<12} {'Coef':>10} {'SE':>10} {'z':>8} {'CI_low':>10} {'CI_up':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for i, name in enumerate(param_names1):\n",
    "    val = theta2[i]\n",
    "    se = se_boot[i]\n",
    "    z = val / se if se > 0 else 0\n",
    "    print(f\"{name:<12} {val:>10.4f} {se:>10.4f} {z:>8.2f} {ci_lower[i]:>10.4f} {ci_upper[i]:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 5. Evaluation Framework Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare beta dictionary and scales for evaluation\n",
    "param_names = get_step2_main_param_names(K=18)\n",
    "beta = dict(zip(param_names, theta2))\n",
    "\n",
    "scales = {\n",
    "    \"tt_scale\": tt_scale,\n",
    "    \"co_scale\": co_scale,\n",
    "    \"he_scale\": he_scale,\n",
    "    \"train_ids\": train_ids,\n",
    "}\n",
    "\n",
    "print(f\"Beta dictionary has {len(beta)} parameters\")\n",
    "print(f\"First 6 params: {list(beta.keys())[:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate real train/test data as baseline\n",
    "train_scored = score_with_baseline_mnl(df_train, beta, scales)\n",
    "test_scored = score_with_baseline_mnl(df_test, beta, scales)\n",
    "\n",
    "train_metrics = downstream_metrics(train_scored)\n",
    "test_metrics = downstream_metrics(test_scored)\n",
    "\n",
    "print(\"Real Train Metrics:\")\n",
    "for k, v in train_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nReal Test Metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Create Templates for Synthetic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create templates for synthetic data generation\n",
    "N = 2000  # Number of synthetic samples\n",
    "p_unseen = 0.2  # Probability of sampling unseen demographic combinations\n",
    "\n",
    "templates = create_templates(\n",
    "    real_train=df_train,\n",
    "    real_test=df_test,\n",
    "    N=N,\n",
    "    p_unseen=p_unseen,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "print(f\"Created {len(templates)} templates\")\n",
    "print(f\"Columns: {list(templates.columns)[:10]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. 路线A: Two-stage CHOICE Generation\n",
    "\n",
    "直接让LLM输出CHOICE(1/2/3)，然后用基准MNL筛查low_prob样本进行修复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route A: Two-stage CHOICE generation\n",
    "# Requires OpenAI API key\n",
    "\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"API key found. Running two-stage generation...\")\n",
    "    \n",
    "    # Define score function for bad_idx detection\n",
    "    def score_fn(df):\n",
    "        return score_with_baseline_mnl(df, beta, scales)\n",
    "    \n",
    "    syn_final = generate_two_stage(\n",
    "        templates,\n",
    "        score_fn=score_fn,\n",
    "        model_stage1=\"gpt-4o-mini\",\n",
    "        model_stage2=\"gpt-4o-mini\",\n",
    "        low_prob_threshold=0.01,\n",
    "        jsonl_dir=\"./\",\n",
    "        seed=123,\n",
    "        cot_stage1=False,\n",
    "        cot_stage2=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated {len(syn_final)} synthetic samples\")\n",
    "    print(\"Choice distribution:\")\n",
    "    print(syn_final[\"CHOICE\"].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_twostage = evaluate_one(syn_final, beta, scales, df_train, df_test, \"two_stage_choice\")\n",
    "    print(\"\\nEvaluation:\")\n",
    "    for k, v in eval_twostage.items():\n",
    "        if 'ds_' in k:\n",
    "            print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set. Skipping LLM generation.\")\n",
    "    print(\"Set: os.environ['OPENAI_API_KEY'] = 'your-key'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. 路线B: Utility + Softmax Generation\n",
    "\n",
    "让LLM输出三个效用值(U_TRAIN, U_SM, U_CAR)，然后用softmax(tau)采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route B: Utility + Softmax with tau sweep\n",
    "\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    taus = [0.5, 1.0, 1.5]  # Adjust as needed\n",
    "    results_u = []\n",
    "    \n",
    "    for tau in taus:\n",
    "        print(f\"\\n--- Utility generation with tau={tau} ---\")\n",
    "        \n",
    "        syn_u = generate_from_utilities_batch(\n",
    "            templates,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            tau=tau,\n",
    "            jsonl_path=f\"./util_tau{tau}.jsonl\",\n",
    "            out_path=f\"./util_tau{tau}_out.jsonl\",\n",
    "            seed=123,\n",
    "            cot=False\n",
    "        )\n",
    "        \n",
    "        # Choice distribution\n",
    "        share = syn_u[\"CHOICE\"].value_counts(normalize=True).reindex([1,2,3]).fillna(0)\n",
    "        print(f\"Choice share: TRAIN={share[1]:.3f}, SM={share[2]:.3f}, CAR={share[3]:.3f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        ev = evaluate_one(syn_u, beta, scales, df_train, df_test, f\"util_tau{tau}\")\n",
    "        ev[\"tau\"] = tau\n",
    "        results_u.append(ev)\n",
    "        \n",
    "        print(f\"ds_avg_P_chosen: {ev['ds_avg_P_chosen']:.4f}\")\n",
    "        print(f\"ds_accuracy: {ev['ds_accuracy']:.4f}\")\n",
    "    \n",
    "    df_results_u = pd.DataFrame(results_u)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(df_results_u[[\"label\", \"tau\", \"ds_avg_P_chosen\", \"ds_accuracy\", \"ds_low_prob_rate(<0.01)\"]])\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. 路线C: MNL + Residual Generation\n",
    "\n",
    "保留基准MNL系统性部分，让LLM生成残差dU ∈ [-1,1]，用logits + λ*dU采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route C: MNL + Residual with lambda sweep\n",
    "\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    lambdas = [0.0, 0.2, 0.3, 0.5]  # 0.0 = pure MNL baseline\n",
    "    results_r = []\n",
    "    \n",
    "    def score_fn(df):\n",
    "        return score_with_baseline_mnl(df, beta, scales)\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        print(f\"\\n--- Residual generation with lambda={lam} ---\")\n",
    "        \n",
    "        syn_r = generate_from_residual_batch(\n",
    "            templates,\n",
    "            score_fn=score_fn,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            lam=lam,\n",
    "            jsonl_path=f\"./resid_lam{lam}.jsonl\",\n",
    "            out_path=f\"./resid_lam{lam}_out.jsonl\",\n",
    "            seed=123,\n",
    "            cot=False\n",
    "        )\n",
    "        \n",
    "        # Choice distribution\n",
    "        share = syn_r[\"CHOICE\"].value_counts(normalize=True).reindex([1,2,3]).fillna(0)\n",
    "        print(f\"Choice share: TRAIN={share[1]:.3f}, SM={share[2]:.3f}, CAR={share[3]:.3f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        ev = evaluate_one(syn_r, beta, scales, df_train, df_test, f\"mnl_plus_resid_lam{lam}\")\n",
    "        ev[\"lambda\"] = lam\n",
    "        results_r.append(ev)\n",
    "        \n",
    "        print(f\"ds_avg_P_chosen: {ev['ds_avg_P_chosen']:.4f}\")\n",
    "        print(f\"ds_low_prob_rate: {ev['ds_low_prob_rate(<0.01)']:.4f}\")\n",
    "    \n",
    "    df_results_r = pd.DataFrame(results_r)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(df_results_r[[\"label\", \"lambda\", \"ds_avg_P_chosen\", \"ds_accuracy\", \"ds_low_prob_rate(<0.01)\"]])\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 10. MNL Baseline (No LLM)\n",
    "\n",
    "作为对照：直接用基准MNL概率采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline synthetic data using MNL probabilities (no LLM)\n",
    "def generate_mnl_baseline(templates, beta, scales, seed=123):\n",
    "    \"\"\"Generate synthetic choices using baseline MNL probabilities.\"\"\"\n",
    "    scored = score_with_baseline_mnl(templates.copy(), beta, scales)\n",
    "    P = scored[[\"P_TRAIN\", \"P_SM\", \"P_CAR\"]].to_numpy()\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    choices = 1 + np.array([rng.choice(3, p=p) for p in P])\n",
    "    \n",
    "    syn = templates.copy()\n",
    "    syn[\"CHOICE\"] = choices\n",
    "    return syn\n",
    "\n",
    "syn_mnl = generate_mnl_baseline(templates, beta, scales, seed=123)\n",
    "\n",
    "print(\"MNL Baseline (no LLM):\")\n",
    "print(\"Choice distribution:\")\n",
    "print(syn_mnl[\"CHOICE\"].value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Evaluate\n",
    "eval_mnl = evaluate_one(syn_mnl, beta, scales, df_train, df_test, \"MNL_baseline\")\n",
    "print(f\"\\nds_avg_P_chosen: {eval_mnl['ds_avg_P_chosen']:.4f}\")\n",
    "print(f\"ds_accuracy: {eval_mnl['ds_accuracy']:.4f}\")\n",
    "print(f\"ds_low_prob_rate: {eval_mnl['ds_low_prob_rate(<0.01)']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 11. 参数稳定性分析 (Real vs Real+Synthetic)\n",
    "\n",
    "比较只用真实数据 vs 用真实+合成数据估计的参数变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter stability analysis\n",
    "# Use the MNL baseline synthetic data for demonstration\n",
    "# In practice, use your best LLM-generated synthetic data (e.g., mnl_plus_resid_lam0.3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Parameter Stability Analysis: Real vs Real+Synthetic\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run stability analysis\n",
    "stab_df, metadata = stability_analysis(\n",
    "    real_train=df_train,\n",
    "    syn_df=syn_mnl,  # Replace with your LLM synthetic data\n",
    "    tt_scale=tt_scale,\n",
    "    co_scale=co_scale,\n",
    "    he_scale=he_scale,\n",
    "    theta_init=theta2,\n",
    "    boot_se=se_boot if 'se_boot' in dir() else None,\n",
    "    ci_lower=ci_lower if 'ci_lower' in dir() else None,\n",
    "    ci_upper=ci_upper if 'ci_upper' in dir() else None,\n",
    "    ratio=1.0,  # 1:1 ratio of synthetic to real\n",
    "    seed=123,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and display results\n",
    "summary = format_stability_for_report(\n",
    "    stab_df,\n",
    "    metadata,\n",
    "    output_path=\"param_stability_real_vs_aug.csv\",\n",
    "    top_n=15\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full table\n",
    "print(\"\\nFull stability table (top 20 by |diff|):\")\n",
    "display_cols = [\"param\", \"coef_base\", \"coef_new\", \"diff\", \"converged_base\", \"converged_new\"]\n",
    "if \"boot_se\" in stab_df.columns:\n",
    "    display_cols.extend([\"boot_se\", \"z_diff_vs_bootse\"])\n",
    "stab_df[display_cols].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 12. Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"Summary Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal parameters: {len(stab_df)}\")\n",
    "print(f\"Max |diff|: {stab_df['abs_diff'].max():.4f} ({stab_df.iloc[0]['param']})\")\n",
    "print(f\"Mean |diff|: {stab_df['abs_diff'].mean():.4f}\")\n",
    "print(f\"Median |diff|: {stab_df['abs_diff'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nConvergence:\")\n",
    "print(f\"  Base (real only): {metadata['converged_base']}\")\n",
    "print(f\"  Augmented (real+syn): {metadata['converged_new']}\")\n",
    "\n",
    "print(f\"\\nLog-likelihood:\")\n",
    "print(f\"  Base: {metadata['ll_base']:.2f}\")\n",
    "print(f\"  Augmented: {metadata['ll_new']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export key results\n",
    "print(\"\\nExported files:\")\n",
    "print(\"  - param_stability_real_vs_aug.csv\")\n",
    "\n",
    "# Export synthetic data if generated\n",
    "if 'syn_final' in dir():\n",
    "    syn_final.to_csv(\"synthetic_llm_two_stage.csv\", index=False)\n",
    "    print(\"  - synthetic_llm_two_stage.csv\")\n",
    "\n",
    "syn_mnl.to_csv(\"synthetic_mnl_baseline.csv\", index=False)\n",
    "print(\"  - synthetic_mnl_baseline.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "### 完成内容\n",
    "\n",
    "1. **数据加载和预处理** - `load_swissmetro()`, `build_matrices()`\n",
    "2. **MNL估计** - Step1(6参数) + Step2(42参数)\n",
    "3. **Bootstrap推断** - 聚类Bootstrap标准误和置信区间\n",
    "4. **评估框架** - `evaluate_one()`, `downstream_metrics()`\n",
    "5. **三条LLM生成路线**:\n",
    "   - A. Two-stage CHOICE\n",
    "   - B. Utility + Softmax\n",
    "   - C. MNL + Residual\n",
    "6. **参数稳定性分析** - Real vs Real+Synthetic\n",
    "\n",
    "### 推荐主结果方案\n",
    "\n",
    "基于 `ds_low_prob_rate=0` 的稳定性，建议使用 **MNL + Residual (λ=0.2或0.3)** 作为主文结果，其他方案放附录。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e770b3e8b821b289"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
